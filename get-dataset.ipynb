{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping - GTD\n",
    "Using the the Global Terrorism Database and Selenium to scrape all data required for research and eventully for the prediction of future terror attacks globally "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Requests Library\n",
    "We will use the bellow mentioned librarys as follows:\n",
    "* Selenium - As our main tool to interact with GTD site and scraping page after page till we collect 50,000 rows of data.\n",
    "* BeautifulSoup - We will load each and every page into a soup object so we can export the data we need.\n",
    "* Pandas - All data we extract from the web page into a Soup object will be stored into a Pandas Dataframe, and eventully be saved to a .csv file\n",
    "* Time - Time will allow us to slow down some of the processes to avoid being blocked from the site thinking we are a bot trying to DDos   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to create a Soup object from the html source page we scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to create a soup obj from a html source\n",
    "def load_html_to_soup(html_source):\n",
    "    bsobj = soup(html_source, 'html.parser')\n",
    "    return bsobj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the source URL and initiate the Selenium webdriver for Chrome browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.start.umd.edu/gtd/search/Results.aspx?page=1&chart=injuries&casualties_type=&casualties_max=&count=100\"\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(URL)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty Dataframe with the columns names of the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Date', 'Country', 'City', 'Perpetrator', 'Fatalities', 'Injured', 'Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "Each page in the website contain 100 rows of data, so we will run 500 times(50,000 rows in total) to get all the data\n",
    "\n",
    "At the end of each page scraping we use the Selenium WebDriver function to move to next page till we reach page 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    try:\n",
    "        table = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"results-table\"))\n",
    "        )\n",
    "    except:\n",
    "        print(\"Exiting with error\")\n",
    "        driver.quit()\n",
    "\n",
    "    bsobj = load_html_to_soup(driver.page_source)\n",
    "    #  Looking for the table with the class 'results'\n",
    "    tableobj = bsobj.find('table', class_='results')\n",
    "    # Collecting Ddata\n",
    "    for row in tableobj.tbody.find_all('tr'):    \n",
    "        columns = row.find_all('td')\n",
    "        if(columns != []):\n",
    "            incident_date = columns[1].text.strip()\n",
    "            countery = columns[2].text.strip()\n",
    "            city = columns[3].text.strip()\n",
    "            perpetrator = columns[4].text.strip()\n",
    "            fatalities = columns[5].text.strip()\n",
    "            injured = columns[6].text.strip()\n",
    "            target = columns[7].text.strip()\n",
    "            df_new_row = pd.DataFrame({\n",
    "                'Date': [incident_date],\n",
    "                'Country': [countery],\n",
    "                'City': [city], \n",
    "                'Perpetrator': [perpetrator], \n",
    "                'Fatalities': [fatalities], \n",
    "                'Injured': [injured], \n",
    "                'Target': [target]\n",
    "            })\n",
    "            df = pd.concat([df, df_new_row], ignore_index=True)\n",
    "    nextResultButton = driver.find_element(by=By.PARTIAL_LINK_TEXT, value=\"MORE RESULTS\")\n",
    "    nextResultButton.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, we save the dataframe as a .csv file and close the Selenium WebDriver session and exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv', mode='a', header=False)\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
